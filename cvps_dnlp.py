# -*- coding: utf-8 -*-
"""CVPS-DNLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U_wjH7ELakg3F5-LEeIu13XrnWfqVp_r

# **Corona Virus Patients Sentiment Analysis using DNLP**
"""

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt

df= pd.read_csv('Corona_NLP_train.csv',encoding='latin1')
df=df.drop(columns=['UserName','ScreenName','Location','TweetAt'])
df=df.dropna(how='any')
df

df['Sentiment'].unique()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
df['Sentiment']= label_encoder.fit_transform(df['Sentiment']) 
df['Sentiment'].unique()

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []
for i in range(0, 41157):
  review = re.sub('[^a-zA-Z]', ' ', df['OriginalTweet'][i])
  review = review.lower()
  review = review.split()
  ps = PorterStemmer()
  all_stopwords = stopwords.words('english')
  all_stopwords.remove('not')
  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]
  review = ' '.join(review)
  corpus.append(review)

len(corpus)

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 1500)
X = cv.fit_transform(corpus).toarray()
y=df.iloc[:,-1].values

print(X)

len(X)

len(y)

print(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

import tensorflow as tf

y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)

y_train

y_test

print("Initialized model")
ann = tf.keras.models.Sequential()
ann.add(tf.keras.layers.Dense(units=500, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1000, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1500, activation='relu'))
ann.add(tf.keras.layers.Dense(units=2000, activation='relu'))
ann.add(tf.keras.layers.Dense(units=5, activation='softmax'))

ann.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = ann.fit(X_train, y_train, batch_size=65, epochs=10, validation_data=(X_test, y_test))
ann.save("CS-DNLP.h5")

plt.figure(0)
plt.plot(history.history['accuracy'], label='training accuracy')
plt.plot(history.history['val_accuracy'], label='val accuracy')
plt.title('Accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()
plt.savefig('Accuracy.png')
plt.figure(1)
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.title('Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.savefig('Loss.png')
print("Saved Model & Graph to disk")

model = tf.keras.models.load_model('CS-DNLP.h5')
print("Loaded model from disk")

DF= pd.read_csv('Corona_NLP_test.csv',encoding='latin1')
DF=DF.drop(columns=['UserName','ScreenName','Location','TweetAt'])
DF=DF.dropna(how='any')
DF

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
DF['Sentiment']=le.fit_transform(DF['Sentiment'])

DF

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
c = []
for i in range(0, 3798):
  rev = re.sub('[^a-zA-Z]', ' ', DF['OriginalTweet'][i])
  rev= rev.lower()
  rev = rev.split()
  ps = PorterStemmer()
  all_stopwords = stopwords.words('english')
  all_stopwords.remove('not')
  rev = [ps.stem(word) for word in rev if not word in set(all_stopwords)]
  rev = ' '.join(rev)
  c.append(rev)

print(c)

len(c)

from sklearn.feature_extraction.text import CountVectorizer
Cv = CountVectorizer(max_features = 1500)
x = Cv.fit_transform(c).toarray()
a = DF.iloc[:, -1].values

x

len(x)

a

a = tf.keras.utils.to_categorical(a)

a

res=model.predict(x)
res=np.round(res)
np.set_printoptions(precision=2)
print(res)

len(res)

from sklearn.metrics import accuracy_score , classification_report
print("Accuracy Score for the algorithm=>{}%".format(round(accuracy_score(a,res)*100),2))